{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T04:06:22.695286Z",
     "start_time": "2025-03-28T04:06:21.639083Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Build path tá»« root\n",
    "IMAGE_DIR = \"../content/cvpr2016_flowers/images\"\n",
    "CAPTION_DIR = \"../content/cvpr2016_flowers/captions\"\n",
    "\n",
    "from load_captions import load_captions\n",
    "\n",
    "captions = load_captions(IMAGE_DIR, CAPTION_DIR)\n",
    "list(captions.items())[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('image_07088', 'dark blue little petals white green and yellow leaves'),\n",
       " ('image_06396',\n",
       "  'this flower is blue and white in color, with petals that are wavy and ruffled.'),\n",
       " ('image_05847', 'the petals of this flower are pink with a short stigma'),\n",
       " ('image_04581',\n",
       "  'a purple and red flower with rolled petals and light green sepal.'),\n",
       " ('image_03588', 'the petals of this flower are pink with a long stigma')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:13:13.379863Z",
     "start_time": "2025-03-28T04:06:25.170752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from encode_captions import encode_captions\n",
    "\n",
    "encoded_caption = encode_captions(captions)\n",
    "print(encoded_caption[\"image_00001\"][\"embed\"].shape)\n",
    "\n"
   ],
   "id": "eb597cbb792faee1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyenhuynh/Documents/text-2-image/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:15:45.158419Z",
     "start_time": "2025-03-28T04:15:45.112302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transform import transform\n",
    "from preprocessing import FlowerDataset\n",
    "\n",
    "dataset = FlowerDataset(IMAGE_DIR, captions=encoded_caption, transform=transform)\n",
    "print(dataset[0][\"image\"].shape)\n",
    "print(dataset[0][\"text\"])"
   ],
   "id": "2618b6c3ed027128",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n",
      "dark blue little petals white green and yellow leaves\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:15:53.956952Z",
     "start_time": "2025-03-28T04:15:53.838206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from model import Generator\n",
    "\n",
    "generator = Generator(noise_size=100, feature_size=128, num_channels=3, embedding_size=768, reduced_dim_size=256)\n",
    "#test\n",
    "batch_size = 5\n",
    "# [batch_size, noise_size]\n",
    "noise = torch.randn(size=(batch_size, 100))\n",
    "print(noise.shape)\n",
    "# [batch_size, embedding_size]\n",
    "embedding_caption = dataset[0][\"embed\"].unsqueeze(0).repeat(batch_size, 1)\n",
    "print(embedding_caption.shape)\n",
    "output = generator(noise, embedding_caption)\n",
    "output.shape\n"
   ],
   "id": "8fee8b18fa1fd2be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100])\n",
      "torch.Size([5, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T04:19:15.500758Z",
     "start_time": "2025-03-28T04:19:15.407667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model import Discriminator\n",
    "\n",
    "images = torch.rand(size=(batch_size, 3, 128, 128))\n",
    "discriminator = Discriminator(3, 128, 768, 256)\n",
    "image = dataset[0][\"image\"].unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "output, _ = discriminator(image, embedding_caption)\n",
    "output.shape"
   ],
   "id": "bfe6ab99563df8db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "510beded19598eb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
